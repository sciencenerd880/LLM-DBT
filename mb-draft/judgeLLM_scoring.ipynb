{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.2\n",
    "# \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\"\n",
    "# \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n",
    "# \"Qwen/Qwen2-VL-72B-Instruct\"\n",
    "# \"scb10x/scb10x-llama3-typhoon-v1-5x-4f316\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import concurrent.futures\n",
    "import re\n",
    "from together import Together\n",
    "from collections import Counter\n",
    "\n",
    "class JudgeLLM:\n",
    "    \"\"\"Judge LLM class to evaluate Shark-Pitch interactions using Together AI's multi-LLM API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None, models=None):\n",
    "        \"\"\"Initialize the Judge LLM with multiple models using Together AI.\"\"\"\n",
    "        self.api_key = api_key or os.getenv('TOGETHER_API_KEY')\n",
    "        self.client = Together(api_key=self.api_key)\n",
    "        self.models = models \n",
    "        print(f\"[DEBUG] Initialized JudgeLLM with models: {self.models}\")\n",
    "    \n",
    "    def generate_response(self, model, messages):\n",
    "        \"\"\"Generate response from Together AI LLM.\"\"\"\n",
    "        print(f\"[DEBUG] Generating response for model: {model}\")\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                stream=False,\n",
    "            )\n",
    "            response = completion.choices[0].message.content if hasattr(completion.choices[0].message, 'content') else str(completion.choices[0].message)\n",
    "            print(f\"[DEBUG] Response received from {model}: {response[:200]}...\")  # Print partial response\n",
    "            return model, response\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error generating response from {model}: {str(e)}\")\n",
    "            return model, f\"Error: {str(e)}\"\n",
    "    \n",
    "    def extract_json_from_response(self, response):\n",
    "        \"\"\"Extracts JSON content from a response that may have additional text.\"\"\"\n",
    "        try:\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                return json.loads(json_str)\n",
    "            else:\n",
    "                print(\"[ERROR] No valid JSON found in response.\")\n",
    "                return {\"reasoning\": response, \"rating of final_offer\": \"Invalid\"}\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"[ERROR] JSON decode failed. Returning raw response.\")\n",
    "            return {\"reasoning\": response, \"rating of final_offer\": \"Invalid\"}\n",
    "    \n",
    "    def ensemble_llm_debate(self, processed_data):\n",
    "        \"\"\"Run multiple LLMs in a two-stage debate format and aggregate their responses for final decision.\"\"\"\n",
    "        print(\"[DEBUG] Starting multi-LLM debate...\")\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a panel of expert venture capitalists analyzing an investment deal based on structured business data. \"\n",
    "            \"You are a panel of expert venture capitalists analyzing the terms of an investment deal based on structured business data. \"\n",
    "            \"You will evaluate key elements including the product details, the Shark LLM's proposed final offer, and the historical actual offer, analysing if the shark's offer is fair to the business on a scale of 1-10, assuming 5 is the original offer accepted by the business. \"\n",
    "            \"The data provided includes: \\n\"\n",
    "            \"- Scenario Name: The unique business pitch scenario. \"\n",
    "            \"- Product Details: Information on the business product details. \"\n",
    "            \"- Product Facts: Financial Information on the business and its offering. \"\n",
    "            \"- Shark LLM Offer: The proposed investment terms made by the Shark LLM. \"\n",
    "            \"- Actual Offer: The real historical investment terms given to this business. \"\n",
    "            \"Each LLM first provides an independent evaluation, then critiques and refines each other's responses before reaching a consensus. \"\n",
    "            \"Return the result in JSON format: \"\n",
    "            '{\"reasoning\": \"Short summary of justification of your score\", \"rating of final_offer\": score (1-10)}'\n",
    "            \"Reasoning is to justify the score given to the final offer, not the summary of the facts or pitch discussion.\"\n",
    "        )\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": json.dumps(processed_data, indent=2, default=str)}]\n",
    "\n",
    "        results = []\n",
    "        individual_scores = []\n",
    "        initial_responses = {}\n",
    "        \n",
    "        # **Step 1: Each LLM Provides an Independent Evaluation**\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future_to_model = {}\n",
    "            for model in self.models:\n",
    "                print(f\"[DEBUG] Submitting request to {model}\")\n",
    "                future_to_model[executor.submit(self.generate_response, model, messages)] = model\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_model):\n",
    "                model, response = future.result()\n",
    "                initial_responses[model] = response  # Save initial reasoning\n",
    "                response_json = self.extract_json_from_response(response)\n",
    "\n",
    "                if \"rating of final_offer\" in response_json and isinstance(response_json[\"rating of final_offer\"], (int, float)):\n",
    "                    individual_scores.append(response_json[\"rating of final_offer\"])\n",
    "                    results.append({\n",
    "                        \"Scenario\": processed_data.get(\"Scenario\", \"Unknown\"),\n",
    "                        \"LLM Model\": model,\n",
    "                        \"Initial Reasoning\": response_json.get(\"reasoning\", \"Not Available\"),\n",
    "                        \"Initial Score\": response_json[\"rating of final_offer\"],  # Changed key\n",
    "                        \"Refined Score\": None,  # Placeholder\n",
    "                        \"Final Consensus Score\": None,  \n",
    "                        \"Final Reasoning\": None  # Placeholder\n",
    "                    })\n",
    "\n",
    "        # **Step 2: Refinement Round - LLMs See Others' Evaluations & Update Scores**\n",
    "        critique_results = []\n",
    "        \n",
    "        for model, initial_response in initial_responses.items():\n",
    "            # Create a prompt that includes all LLM evaluations for comparison\n",
    "            critique_prompt = (\n",
    "                \"You are an expert VC evaluating a deal. Here are multiple analyses from different experts:\\n\\n\"\n",
    "                f\"{json.dumps(initial_responses, indent=2)}\\n\\n\"\n",
    "                \"Your task: Critique and refine your own response in light of these evaluations. \"\n",
    "                \"If convinced by another argument, update your score. Otherwise, justify why your score remains the same.\"\n",
    "                \"Return the result in JSON format: \"\n",
    "                '{\"final_reasoning\": \"Final refined evaluation\", \"updated_rating\": score (1-10)}'\n",
    "            )\n",
    "\n",
    "            critique_messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": critique_prompt}]\n",
    "\n",
    "            # Get critique response\n",
    "            print(f\"[DEBUG] Get critique response from {model}\")\n",
    "            model, critique_response = self.generate_response(model, critique_messages)\n",
    "            critique_json = self.extract_json_from_response(critique_response)\n",
    "            \n",
    "            if \"updated_rating\" in critique_json and isinstance(critique_json[\"updated_rating\"], (int, float)):\n",
    "                critique_results.append({\n",
    "                    \"Scenario\": processed_data.get(\"Scenario\", \"Unknown\"),\n",
    "                    \"LLM Model\": model,\n",
    "                    \"Refined Score\": critique_json[\"updated_rating\"],  \n",
    "                    \"Final Consensus Score\": None,  # Added comma\n",
    "                    \"Final Reasoning\": critique_json.get(\"final_reasoning\", \"Not Available\")\n",
    "                })\n",
    "\n",
    "        # **Step 3: Compute Final Consensus Score**\n",
    "        final_scores = [result[\"Refined Score\"] for result in critique_results if isinstance(result[\"Refined Score\"], (int, float))]\n",
    "        consensus_score = self.majority_vote(final_scores)\n",
    "        print(f\"[DEBUG] Final consensus score computed: {consensus_score}\")\n",
    "\n",
    "        # Update results with refined scores and final consensus\n",
    "        for result, critique in zip(results, critique_results):\n",
    "            result[\"Refined Score\"] = critique[\"Refined Score\"]\n",
    "            result[\"Final Consensus Score\"] = consensus_score\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    \n",
    "    def majority_vote(self, scores):\n",
    "        \"\"\"Determine the consensus decision using majority voting or averaging scores.\"\"\"\n",
    "        if not scores:\n",
    "            return \"Not Available\"\n",
    "        return round(sum(scores) / len(scores), 2)\n",
    "    \n",
    "    def judge_scoring(self, processed_data):\n",
    "        \"\"\"Evaluate the final offer comparison\"\"\"\n",
    "        print(\"[DEBUG] Starting judge_scoring function...\")\n",
    "        return self.ensemble_llm_debate(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Initialized JudgeLLM with models: ['deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free', 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free', 'scb10x/scb10x-llama3-typhoon-v1-5x-4f316']\n",
      "[DEBUG] Starting judge_scoring function...\n",
      "[DEBUG] Starting multi-LLM debate...\n",
      "[DEBUG] Submitting request to deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\n",
      "[DEBUG] Generating response for model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\n",
      "[DEBUG] Submitting request to meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\n",
      "[DEBUG] Generating response for model: meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\n",
      "[DEBUG] Submitting request to scb10x/scb10x-llama3-typhoon-v1-5x-4f316\n",
      "[DEBUG] Generating response for model: scb10x/scb10x-llama3-typhoon-v1-5x-4f316\n",
      "[DEBUG] Response received from scb10x/scb10x-llama3-typhoon-v1-5x-4f316: {\"reasoning\": \"The Shark LLM's offer is significantly more favorable than the actual offer, with a higher investment amount and lower equity stake. The business's strong sales growth and distribution ...\n",
      "[DEBUG] Response received from meta-llama/Llama-3.3-70B-Instruct-Turbo-Free: To evaluate the fairness of the Shark LLM's proposed final offer, we need to consider several key factors, including the product's market potential, sales history, distribution network, costs, pricing...\n",
      "[DEBUG] Response received from deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free: <think>\n",
      "Okay, so I'm trying to figure out whether the Shark LLM's offer for TouchUp Cup is fair. Let me start by understanding the business. TouchUp Cup is a product designed to solve paint storage is...\n",
      "[DEBUG] Get critique response from scb10x/scb10x-llama3-typhoon-v1-5x-4f316\n",
      "[DEBUG] Generating response for model: scb10x/scb10x-llama3-typhoon-v1-5x-4f316\n",
      "[DEBUG] Response received from scb10x/scb10x-llama3-typhoon-v1-5x-4f316: {\"final_reasoning\": \"After reviewing the analyses from other experts, I agree that the Shark LLM's offer is more favorable than the actual offer due to its higher investment amount and lower equity st...\n",
      "[DEBUG] Critique response from scb10x/scb10x-llama3-typhoon-v1-5x-4f316: {'final_reasoning': \"After reviewing the analyses from other experts, I agree that the Shark LLM's offer is more favorable than the actual offer due to its higher investment amount and lower equity stake. The company's strong sales growth, extensive distribution network, and patented products justify a higher valuation. However, the additional 2% equity kicker if sales exceed $1 million in the next 12 months adds some risk to the deal. Overall, the Shark LLM's offer is fair to the business, but the complexity and potential for future dilution should be considered. I maintain my initial rating of 8 out of 10.\", 'updated_rating': 8}\n",
      "[DEBUG] Get critique response from meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\n",
      "[DEBUG] Generating response for model: meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\n",
      "[DEBUG] Response received from meta-llama/Llama-3.3-70B-Instruct-Turbo-Free: After reviewing the evaluations from other experts, I notice a consensus that the Shark LLM's offer is more favorable to the business compared to the actual historical offer. The key points highlighte...\n",
      "[DEBUG] Critique response from meta-llama/Llama-3.3-70B-Instruct-Turbo-Free: {'final_reasoning': \"The Shark LLM's offer is fair, considering the company's strong growth, distribution network, and product line. The offer provides a better valuation and retains more equity for the founders, but the 2% equity kicker introduces complexity and potential future dilution. The balance between the favorable terms and potential risks justifies the rating.\", 'updated_rating': 8}\n",
      "[DEBUG] Get critique response from deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\n",
      "[DEBUG] Generating response for model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\n",
      "[DEBUG] Response received from deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free: <think>\n",
      "Alright, I'm trying to evaluate whether the Shark LLM's offer for TouchUp Cup is fair. Let me go through the details step by step.\n",
      "\n",
      "First, the product itself seems innovative. It's solving a r...\n",
      "[DEBUG] Critique response from deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free: {'final_reasoning': \"The Shark LLM's offer is fair as it provides a substantial investment with a reasonable equity stake, considering the company's growth, distribution, and product line. The 2% kicker adds complexity but aligns interests, making the offer favorable overall.\", 'updated_rating': 8}\n",
      "[DEBUG] Final consensus score computed: 8.0\n"
     ]
    }
   ],
   "source": [
    "llm_models = [\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\", \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", \"scb10x/scb10x-llama3-typhoon-v1-5x-4f316\"]\n",
    "judge = JudgeLLM(api_key=TOGETHERAI_API_KEY, models=llm_models)\n",
    "\n",
    "csv_file = 'shark_pitch_data_cleaned.csv'\n",
    "processed_df = pd.read_csv(csv_file)\n",
    "#1st 6 rows\n",
    "sampled_df = processed_df[:1]   \n",
    "\n",
    "# Update main execution loop:\n",
    "final_results = []\n",
    "for _, row in sampled_df.iterrows():\n",
    "    scenario_data = row.to_dict()\n",
    "    result_df = judge.judge_scoring(scenario_data)  # Remove CSV parameter\n",
    "    final_results.append(result_df)\n",
    "\n",
    "# Single save after processing all rows\n",
    "all_results_df = pd.concat(final_results, ignore_index=True)\n",
    "all_results_df.to_csv(\"final_judge_results_v3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharktank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
