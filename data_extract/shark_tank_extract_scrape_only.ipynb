{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting selenium\n",
      "  Using cached selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting webdriver_manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting openai\n",
      "  Downloading openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting yt_dlp\n",
      "  Using cached yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\derek\\documents\\github\\shark_tank\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting urllib3<3,>=1.26 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Using cached trio_websocket-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2021.10.8 (from selenium)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions~=4.9 (from selenium)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting python-dotenv (from webdriver_manager)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\derek\\documents\\github\\shark_tank\\.venv\\lib\\site-packages (from webdriver_manager) (24.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\derek\\documents\\github\\shark_tank\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\derek\\documents\\github\\shark_tank\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading openai-1.64.0-py3-none-any.whl (472 kB)\n",
      "Using cached yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl (204 kB)\n",
      "Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "Using cached trio_websocket-0.12.1-py3-none-any.whl (21 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, pytz, yt_dlp, websocket-client, urllib3, tzdata, typing_extensions, tqdm, soupsieve, sniffio, python-dotenv, pysocks, pycparser, numpy, jiter, idna, h11, et-xmlfile, distro, charset-normalizer, certifi, attrs, annotated-types, wsproto, requests, pydantic-core, pandas, outcome, openpyxl, httpcore, cffi, beautifulsoup4, anyio, webdriver_manager, trio, pydantic, httpx, bs4, trio-websocket, openai, selenium\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.8.0 attrs-25.1.0 beautifulsoup4-4.13.3 bs4-0.0.2 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.1 distro-1.9.0 et-xmlfile-2.0.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.8.2 numpy-2.2.3 openai-1.64.0 openpyxl-3.1.5 outcome-1.3.0.post0 pandas-2.2.3 pycparser-2.22 pydantic-2.10.6 pydantic-core-2.27.2 pysocks-1.7.1 python-dotenv-1.0.1 pytz-2025.1 requests-2.32.3 selenium-4.29.0 sniffio-1.3.1 sortedcontainers-2.4.0 soupsieve-2.6 tqdm-4.67.1 trio-0.29.0 trio-websocket-0.12.1 typing_extensions-4.12.2 tzdata-2025.1 urllib3-2.3.0 webdriver_manager-4.0.2 websocket-client-1.8.0 wsproto-1.2.0 yt_dlp-2025.2.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas selenium webdriver_manager requests bs4 openai yt_dlp openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subslikescript.com/series/Shark_Tank-1442550/season-12/episode-1-Episode_121\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-12/episode-2-Episode_122\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-12/episode-6-Episode_126\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-12/episode-9-Episode_129\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-12/episode-11-Episode_1211\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-12/episode-23-Episode_1223\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-12/episode-24-Episode_1224\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-12/episode-25-Episode_1225\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-4-Episode_134\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-5-Episode_135\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-6-Episode_136\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-7-Episode_137\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-8-Episode_138\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-9-Episode_139\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-10-Episode_1310\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-11-Episode_1311\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-12-Episode_1312\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-13-Episode_1313\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-15-Episode_1315\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-16-Episode_1316\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-13/episode-23-Episode_1323\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-1-Episode_141\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-2-Episode_142\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-3-Episode_143\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-4-Episode_144\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-5-Episode_145\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-6-Episode_146\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-7-Episode_147\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-8-Episode_148\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-9-Episode_149\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-15-Episode_1415\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-16-Episode_1416\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-17-Episode_1417\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-18-Episode_1418\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-19-Episode_1419\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-20-Episode_1420\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-14/episode-21-Episode_1421\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-15/episode-6-Episode_156\n",
      "https://subslikescript.com/series/Shark_Tank-1442550/season-15/episode-7-Episode_157\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL you want to scrape\n",
    "url = 'https://subslikescript.com/series/Shark_Tank-1442550'\n",
    "\n",
    "# Fetch the content from the URL\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all anchor tags and extract the 'href' attribute\n",
    "urls = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "\n",
    "# Filter the URLs to only include the ones that start with '/series/Shark_Tank'\n",
    "urls = [f'https://subslikescript.com{link}' for link in urls if link.startswith('/series/Shark_Tank')]\n",
    "# Print the list of URLs\n",
    "for link in urls:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_list = []\n",
    "\n",
    "for link in urls:\n",
    "    response = requests.get(link)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    script = soup.find('div', class_='full-script')\n",
    "\n",
    "    if script:\n",
    "        for br in script.find_all(\"br\"):\n",
    "            br.replace_with(\"\\n\")\n",
    "    \n",
    "        script_text = script.get_text() if script else 'No script found'\n",
    "        extract_list.append({'url': link, 'script': script_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(extract_list)\n",
    "df.to_excel('shark_tank_scripts.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df)):\n",
    "    filename = df['url'][i].split('/')[-1]\n",
    "    with open(f'raw_web_scrape/sn_{i}_shark_tank_script_{filename}.txt', 'w',encoding='utf-8') as f:\n",
    "        f.write(df['script'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sn_0_shark_tank_script_episode-1-Episode_121.txt\n",
      "sn_10_shark_tank_script_episode-6-Episode_136.txt\n",
      "sn_11_shark_tank_script_episode-7-Episode_137.txt\n",
      "sn_12_shark_tank_script_episode-8-Episode_138.txt\n",
      "sn_13_shark_tank_script_episode-9-Episode_139.txt\n",
      "sn_14_shark_tank_script_episode-10-Episode_1310.txt\n",
      "sn_15_shark_tank_script_episode-11-Episode_1311.txt\n",
      "sn_16_shark_tank_script_episode-12-Episode_1312.txt\n",
      "sn_17_shark_tank_script_episode-13-Episode_1313.txt\n",
      "sn_18_shark_tank_script_episode-15-Episode_1315.txt\n",
      "sn_19_shark_tank_script_episode-16-Episode_1316.txt\n",
      "sn_1_shark_tank_script_episode-2-Episode_122.txt\n",
      "sn_20_shark_tank_script_episode-23-Episode_1323.txt\n",
      "sn_21_shark_tank_script_episode-1-Episode_141.txt\n",
      "sn_22_shark_tank_script_episode-2-Episode_142.txt\n",
      "sn_23_shark_tank_script_episode-3-Episode_143.txt\n",
      "sn_24_shark_tank_script_episode-4-Episode_144.txt\n",
      "sn_25_shark_tank_script_episode-5-Episode_145.txt\n",
      "sn_26_shark_tank_script_episode-6-Episode_146.txt\n",
      "sn_27_shark_tank_script_episode-7-Episode_147.txt\n",
      "sn_28_shark_tank_script_episode-8-Episode_148.txt\n",
      "sn_29_shark_tank_script_episode-9-Episode_149.txt\n",
      "sn_2_shark_tank_script_episode-6-Episode_126.txt\n",
      "sn_30_shark_tank_script_episode-15-Episode_1415.txt\n",
      "sn_31_shark_tank_script_episode-16-Episode_1416.txt\n",
      "sn_32_shark_tank_script_episode-17-Episode_1417.txt\n",
      "sn_33_shark_tank_script_episode-18-Episode_1418.txt\n",
      "sn_34_shark_tank_script_episode-19-Episode_1419.txt\n",
      "sn_35_shark_tank_script_episode-20-Episode_1420.txt\n",
      "sn_36_shark_tank_script_episode-21-Episode_1421.txt\n",
      "sn_37_shark_tank_script_episode-6-Episode_156.txt\n",
      "sn_38_shark_tank_script_episode-7-Episode_157.txt\n",
      "sn_3_shark_tank_script_episode-9-Episode_129.txt\n",
      "sn_4_shark_tank_script_episode-11-Episode_1211.txt\n",
      "sn_5_shark_tank_script_episode-23-Episode_1223.txt\n",
      "sn_6_shark_tank_script_episode-24-Episode_1224.txt\n",
      "sn_7_shark_tank_script_episode-25-Episode_1225.txt\n",
      "sn_8_shark_tank_script_episode-4-Episode_134.txt\n",
      "sn_9_shark_tank_script_episode-5-Episode_135.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "prefixes = ['Narrator: First in', 'First in','Next up', 'Narrator: Next up','Next in', 'Narrator: Next in']\n",
    "\n",
    "for file in os.listdir('raw_web_scrape'):\n",
    "    list_of_lines = []\n",
    "    if file.endswith('.txt'):\n",
    "        with open(f'raw_web_scrape/{file}', 'r', encoding='utf-8') as f:\n",
    "            print(file)\n",
    "            text = f.read()\n",
    "            lines = text.split('\\n')\n",
    "            lines = [l for l in lines if l and l.strip() !='Narrator:' and l.strip() !='♪♪'] \n",
    "            \n",
    "            for i,line  in enumerate(lines):\n",
    "                if line.startswith(tuple(prefixes)):\n",
    "                    list_of_lines.append(i)\n",
    "\n",
    "            list_of_lines.append(len(lines))\n",
    "\n",
    "            for i in range(0, len(list_of_lines)-1):\n",
    "                with open(f'split_web_scrape/{file}_part_{i}', 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(lines[list_of_lines[i]:list_of_lines[i+1]]) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
